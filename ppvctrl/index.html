<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="PP-VCtrl is a controllable video generation model that uses an auxiliary condition encoder to transform a text-to-video generation model into a custom video generator, without retraining of the original generator.">
  <meta property="og:title" content="PP-VCtrl:Controllable Video Generation Model Based on Text-to-Video Diffusion Model"/>
  <meta property="og:description" content="PP-VCtrl is a controllable video generation model that uses an auxiliary condition encoder to transform a text-to-video generation model into a custom video generator, without retraining of the original generator."/>
  <meta property="og:url" content="https://hammingbo.github.io/ppvctrl"/>
  <meta property="og:image" content="static/image/og_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="PP-VCtrl:Controllable Video Generation Model Based on Text-to-Video Diffusion Model">
  <meta name="twitter:description" content="PP-VCtrl is a controllable video generation model that uses an auxiliary condition encoder to transform a text-to-video generation model into a custom video generator, without retraining of the original generator.">
  <meta name="twitter:image" content="static/images/twitte_model.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="Video diffusion models, controllable video generation, image-to-video, text-to-video">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>PPVCtrl: Controllable Video Generation Model Based on Text-to-Video Diffusion Model  </title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">
  <link rel="icon" type="image/png" href="static/images/image.jpg">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">PP-VCtrl: Controllable Video Generation Model Based on Text-to-Video Diffusion Model</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
             <!-- <span class="author-block">
                Eyal Molad<sup>*,1</sup>,
              </span> -->
              <!-- <span class="author-block">
                <a href="https://pages.cs.huji.ac.il/eliahu-horwitz/" target="_blank">Eliahu Horwitz</a><sup>*,1,2</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/dani-valevski-a3b5936/" target="_blank">Dani Valevski</a><sup>*,1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/alexravacha" target="_blank">Alex Rav Acha</a><sup>1</sup>,
              </span>
              <span class="author-block">
                Yossi Matias<sup>1</sup>, -->
              <!-- </span>
              <span class="author-block">
                Yael Pritch<sup>1</sup>,
              </span>
              <span class="author-block">
                 <a href="https://yanivle.github.io/" target="_blank">Yaniv Leviathan</a><sup>†,1</sup>, -->
              <!-- </span>
              <span class="author-block">
                <a href="http://www.cs.huji.ac.il/~ydidh" target="_blank">Yedid Hoshen</a><sup>†,1,2</sup>
              </span>
            </div> -->

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Baidu Inc</span>
            </div>

            <!-- <div class="column has-text-centered">
              <div class="publication-links"> -->
                <!-- Arxiv PDF link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/pdf/2302.01329.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span> -->
              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
            </span>



            <!-- ArXiv abstract Link -->
            <!-- <span class="link-block">
              <a href="https://arxiv.org/abs/2302.01329" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
              </a>
            </span> -->
          </div>
        </div>
      </div>
    </div>
  </div>
</div>
</section>




<!-- <h1 style="text-align: center; font-size: 18px; margin-bottom: 20px;">
  We propose PP-Vctrl, a controllable video generation model that uses an auxiliary condition encoder to transform a text-to-video or image-to-video generation model into a custom video generator, without retraining of the original generator.
</h1> -->

<h2 class="title is-3" style="text-align: center; margin-top: 20px;">DEMOS SHOW</h2>

<!-- Canny Video Generation carousel -->
<section class="hero teaser">
      <div class="hero-body ">
    <div class="container is-max-desktop">
      
    <div id="results-carousel" class="carousel results-carousel">
        
        <div class="item item-video1">
          <video poster="" id="header-video1" autoplay controls muted loop height="100%">
            <source src="static/videos/canny_longvideo_case1.mp4"
            type="video/mp4">
          </video>
        <h2 class="subtitle has-text-centered",style="font-weight: bold">
          " PPVCtrl-Canny can effortlessly assist creators in achieving style transfer from video to artwork by leveraging the video's edge features as control conditions."
        </h2>
        </div>

        <div class="item item-video2">
          <video poster="" id="header-video2" autoplay controls muted loop height="100%">
            <source src="static/videos/canny_longvideo_case2.mp4"
            type="video/mp4">
          </video>
        <h2 class="subtitle has-text-centered">
          "PPVCtrl-Canny can effortlessly assist creators in achieving style transfer from anime-style videos to real-world videos by leveraging the video's edge features as control conditions."
        </h2>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End-->

<!--  Mask Video Generation carousel -->
<section class="hero teaser">
      <div class="hero-body ">
    <div class="container is-max-desktop">
      
    <div id="results-carousel" class="carousel results-carousel">
        
        <div class="item item-video1">
          <video poster="" id="header-video1" autoplay controls muted loop height="100%">
            <source src="static/videos/mask_longvideo_case1.mp4"
            type="video/mp4">
          </video>
        <h2 class="subtitle has-text-centered",style="font-weight: bold">
          " PPVCtrl-Mask allows creators to easily perform diverse video editing tasks by selecting the specific content to be edited, enabling efficient customization."
        </h2>
        </div>

        <div class="item item-video2">
          <video poster="" id="header-video2" autoplay controls muted loop height="100%">
            <source src="static/videos/mask_longvideo_case2.mp4"
            type="video/mp4">
          </video>
        <h2 class="subtitle has-text-centered">
          " PPVCtrl-Mask allows creators to easily perform diverse video editing tasks by selecting the specific content to be edited, enabling efficient customization."
        </h2>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End-->


<!--  Pose Video Generation carousel -->
<section class="hero teaser">
  <div class="hero-body ">
<div class="container is-max-desktop">
  
<div id="results-carousel" class="carousel results-carousel">
  <div class="item item-video1">
    <video poster="" id="header-video1" autoplay controls muted loop height="100%">
      <source src="static/videos/pose_case_10.mp4"
      type="video/mp4">
    </video>
  <h2 class="subtitle has-text-centered",style="font-weight: bold">
    "PPVCtrl-Pose enables creators to effortlessly achieve customized generation of character motion videos by extracting pose conditions."
  </h2>
  </div>
    
    <div class="item item-video1">
      <video poster="" id="header-video1" autoplay controls muted loop height="100%">
        <source src="static/videos/pose_case_11.mp4"
        type="video/mp4">
      </video>
    <h2 class="subtitle has-text-centered",style="font-weight: bold">
      "PPVCtrl-Pose enables creators to effortlessly achieve customized generation of character motion videos by extracting pose conditions."
    </h2>
    </div>
  </div>
</div>
</div>
</section>

<!-- Canny v2v -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container  is-max-desktop">
      <h2 class="title is-3" style="text-align: center;">PPVCtrl-Canny-I2V</h2>
      <p class="subtitle" style="text-align: left; margin-top: 10px;">
        First, you should perform Canny edge detection on the Source Video to obtain the corresponding Control Video. 
        Then, use ControlNet-Canny to redraw the first frame of the video to obtain the Reference Image. 
        Finally, use the Control Video and Reference Image as conditions to generate a stylistically different video.
      </p>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="vid2vid_video1" playsinline autoplay muted loop height="90%" src="static/videos/canny_case1.mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="vid2vid_video2" playsinline autoplay muted loop height="90%" src="static/videos/canny_case2.mp4">
          </video>
        </div>
        </div>
    </div>
  </div>
</section>
<!-- End Canny v2v -->

<!-- Mask v2v -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container  is-max-desktop">
      <h2 class="title is-3" style="text-align: center;">PPVCtrl-Mask-I2V</h2>
      <p class="subtitle" style="text-align: left; margin-top: 10px;">
        First, you are expected to select the subject to be edited from the source video and use SAM2 for subject segmentation to obtain the control video. 
        Then, use Image Inpainting to edit the first frame of the video to obtain the reference image. 
        Finally, use the control video and reference image as conditions to edit the entire video.
      </p>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="vid2vid_video1" playsinline autoplay muted loop height="90%" src="static/videos/mask_case1.mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="vid2vid_video2" playsinline autoplay muted loop height="90%" src="static/videos/mask_case2.mp4">
          </video>
        </div>
        </div>
    </div>
  </div>
</section>
<!-- End Mask v2v -->

<!-- Pose v2v -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container  is-max-desktop">
      <h2 class="title is-3" style="text-align: center;">PPVCtrl-Pose-I2V</h2>
      <p class="subtitle" style="text-align: left; margin-top: 10px;">
        First, you can use a pose detection model to obtain the sequence of poses from the source video. 
        Then, use ControlNet-Pose to regenerate the first frame of the video, obtaining the reference image. 
        Finally, use the control video and reference image as conditions to perform style transfer and redraw the video.
      </p>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="vid2vid_video1" playsinline autoplay muted loop height="90%" src="static/videos/pose_case1.mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="vid2vid_video2" playsinline autoplay muted loop height="90%" src="static/videos/pose_case2.mp4">
          </video>
        </div>
        </div>
    </div>
  </div>
</section>
<!-- End Pose v2v -->




<!-- Paper abstract -->
<!-- <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Text-driven image and video diffusion models have recently achieved unprecedented generation realism. While diffusion models have been successfully applied for image editing, very few works have done so for video editing. We present the first diffusion-based method that is able to perform text-based motion and appearance editing of general videos. Our approach uses a video diffusion model to combine, at inference time, the low-resolution spatio-temporal information from the original video with new, high resolution information that it synthesized to align with the guiding text-prompt. As obtaining high-fidelity to the original video requires retaining some of its high-resolution information, we add a preliminary stage of finetuning the model on the original video, significantly boosting fidelity. We propose to improve motion editability by a new, mixed objective that jointly finetunes with full temporal attention and with temporal attention masking. We further introduce a new framework for image animation. We first transform the image into a coarse video by simple image processing operations such as replication and perspective geometric projections, and then use our general video editor to animate it. As a further application, we can use our method for subject-driven video generation. Extensive qualitative and numerical experiments showcase the remarkable editing ability of our method and establish its superior performance compared to baseline methods.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End paper abstract -->

<!-- <section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Method Overview: Mixed Video-Image Finetuning</h2>
          <center>
          <img src="static/images/architecture_finetune.png" alt="Mixed Video-Image Finetuning" class="center-image blend-img-background"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
               Finetuning the video diffusion model on the input video alone limits the extent of motion change. Instead, we use a mixed objective that beside the original objective (bottom left) also finetunes on the unordered set of frames. This is done by using “masked temporal attention”, preventing the temporal attention and convolution from being finetuned (bottom right). This allows adding motion to a static video.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->



<!-- <section class="section is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Inference Overview</h2>
          <center>
          <img src="static/images/architecture_applications_extended.png" alt="Inference Overview" class="center-image"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
               Our method supports multiple applications by application dependent pre-processing (left), converting the input content into a uniform video format. For image-to-video, the input image is duplicated and transformed using perspective transformations, synthesizing a coarse video with some camera motion. For subject driven video generation, the input is omitted -  finetuning alone take care of the fidelity.
               This coarse video is then edited using our general ``Dreamix Video Editor`` (right): we first corrupt the video by downsampling followed by adding noise. We then apply the finetuned text-guided video diffusion model, which upscales the video to the final spatio-temporal resolution
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->



<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/xcvnHhfDSGM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->



<!--BibTex citation -->
<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{molad2023dreamix,
  title={Dreamix: Video Diffusion Models are General Video Editors},
  author={Molad, Eyal and Horwitz, Eliahu and Valevski, Dani and Acha, Alex Rav and Matias, Yossi and Pritch, Yael and Leviathan, Yaniv and Hoshen, Yedid},
  journal={arXiv preprint arXiv:2302.01329},
  year={2023}
}</code></pre>
  </div>
</section> -->
<!-- End BibTex citation -->

<!-- Acknowledgements
<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
  We thank Ely Sarig for creating the video, Jay Tenenbaum for the video narration, Amir Hertz for the implementation of our eval baseline, Daniel Cohen-Or, Assaf Zomet, Eyal Segalis, Matan Kalman and Emily Denton for their valuable inputs that helped improve this work.
  </div>
</section> -->
<!--End Acknowledgements -->
  
<!-- <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer> -->

<!-- Statcounter tracking code -->

<!-- Default Statcounter code for Dreamix
https://dreamix-video-editing.github.io -->
<!-- <script type="text/javascript">
var sc_project=12843789; 
var sc_invisible=1; 
var sc_security="e9c3bf5f"; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img
class="statcounter"
src="https://c.statcounter.com/12843789/0/e9c3bf5f/1/"
alt="Web Analytics"
referrerPolicy="no-referrer-when-downgrade"></a></div></noscript> -->

<!-- End of Statcounter Code -->

</body>
</html>
